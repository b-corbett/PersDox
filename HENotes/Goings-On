For current math solving testing & report
1/19
William: Did you fix that bug you were showing me last time?
	 Also im going to need access to your repository https://vcs.he.net/math-solver/math-site-solvers-brian.corbett/-/tree/math_solver_report and not
	    just for a temporary amount of time, please either move it to a repository under your own name so you can give me access yourself or tell tom
	    to not put a time restriction on it, if you have trouble let me know and ill talk to him about it
Me: Hey will, I made my own repository https://vcs.he.net/brian.corbett/mathbot_testing and i invited you to have maintainer permissions indefinitely. i've
    fixed all bugs associated with creating json tests and running the tests. I'm still working on the overall test results. I've had to refactor a couple of 
    times because the way I was trying to parse all the results was turning out to be spaghetti code. I've now got a plan to continue with the overall test 
    results. 
    a couple questions:
    so far im thinking pass/fail results for each category (basic_math, algebra, etc.) of problems (which i'm about halfway done with already), 
                   and pass/fail results for each problem specifically which are actually calculated in each problems' folder already, should I also show
		   those results in the overall test result file?
    are  you planning on using the math generator tester as soon as possible? 
    The tester itself and adding a test result file for each individual problem is working so that's usable. I just can't think of much else I can measure 
    for the overall results besides what is mentioned above, but what other metrics do you have in mind? you name it and I'll include it 
William: im hoping for a way to distinguish between solvers that have tagged output and fail the tests vs solvers that don't have tagged output yet vs 
	 problems that we dont even have solvers for yet so those 3 different cases of failing should be tallied/indicated somehow also can you put in the
	 readme of the repository how to 1) write a manual test for a solver 2) run all the tests and where the output is 3) run any other script thats 
	 part of the project if any, plus add any other details to the readme that you think is important
Me: Hey Will, I've made some progress with determining which problems are tagged and which have solvers. 
    Essentially if they have a solver and have tagged output, then they will just either pass or fail and the test results file will show that. 
    If they have a solver but don't have tagged output, the error message will say "no tags with 'he-info' found"
    If the problem is unparseable, this would either mean the input to the mathbot was invalid (even if there is a solver for it) or there is no solver for
    it (correct me if i'm wrong on that). The test results will show that with a somewhat cryptic message at the moment so i will make it clearer, but it 
    is associated with not being able to parse. 
    So at the moment all three scenarios seem to be accounted for, based on the testing i've done so far. I just need to tally the results and put them 
    into the overall results. 
    I did find a bit of an issue with the mathgenerator however. some questions that are generated are actually unparseable to the mathbot even though 
    they have solvers for it. for example:
    the mathgenerator generates problems for the least common multiple,
	here's an example of one: LCM of $7$ and $20 =$
    if you put that into the mathbot site it will not be able to parse it and running it through my tester will show it as not having a solver for it / not
    being parseable. But least common multiple actually does have a solver for it. 
        for example if you remove the dollar and equal signs and enter this into mathbot: LCM of 7 and 20
    it will provide the correct answer. and if i change the questions in the json files that the mathgenerator produced, then the test results will come 
    back that the solution isn't tagged, which is correct in this case. 
    so all this just to let you know that, as far as i've tested, my program will differentiate between the different scenarios you mentioned, but at 
    least 1 (and most likely more than 1) of the problems that are generated by the mathgenerator are are not produced in a valid format that the mathbot 
    can parse which will effect the accuracy of the overall test results.  
William: thanks for the update
	 for those problems that you find need to be changed to work with mathbot, could you report that manually somewhere, could just be in some file
	 alternatively id like if you implemented some testing for common variations that might work; for example, if a problem doesnt parse initially, 
	 but removing dollar signs (or some other common pattern change) fixes it, report that too (not sure if theres any other generic transformation 
	 that might make problems parseable, but removing dollar signs is pretty common)
	 thanks again and let me know when youve added the tally
    2/7 - 2/8
William: i can get to testing the scripts in a bit
	 but id like you to make a web interface that you can manage, add, run, and summarize tests from
	 it should be able to execute all the same functions as the script
	 the main feature would be to have a page to run all the tests and then display the results, per question, as well as an overall 
	 summary
	 tests that are previously ran should be saved, and be viewable again from the web interface
	 ideally you should have the web interface running on your staff.he.net virtual host, which you can ask james on how to log in to 
	 that and how to make the web interface public
	 which means the frontend should be written in php
	 i can write up a more formal spec if you need
	 but if you just start by getting the web interface in php running on localhost on your machine thats a good start
William: still writing the spec but just to clarify some things before you leave today:
	 the web interface should be written in php such that it can be run on the apache web server on staff.he.net, which you have credentials to ssh 
	 into. for the action of triggering all the tests to run, you should use semaphores or mutexes in php to insure that this can only be triggered 
	 once at a time (two users should not simultaneously both be able to trigger the tests to run)
	 there is an sqlite database on the server, so whatever data you might want to store in a database, use sqlite





1/18
Robert: The return code being 0 when 0 tests were found is the correct behavior as originally intended, but I agree it is a little confusing. I will move it to 2. I'm thinking of reserving 1 for "tests were found correctly, but failed" and 2 as a generic "something happened, and tests were not found/loaded correctly."
        ok, the latest commit to master should reflect those changes. Addition.json's question is just "2+2", which isn't tagged yet.

	service@service:~/mathbot_api_test$ python3 test.py addition.json -v
	WARNING:root:the string 'he-info' was not found in the html_solution returned by the mathbot API.
	ERROR:root:No tests were run. Either there were 0 tests found in test.json, or there was no HTML element tagged with an id starting with "he-info" in the API response's "html_solution" field.
	service@service:~/mathbot_api_test$ echo $?
	2
	we do have a full 255 error codes available to use, with around ~130 Linux-standard ones: https://chromium.googlesource.com/chromiumos/docs/+/HEAD/constants/errnos.md
	If it would be helpful to have your program do something based on the specific return code you get back, I can implement more than just 0,1 and 2.



1/3
William: great so just to confirm you should have the tests in such a way that they can be batch tested so we can check the changes per problem on whether 
				 our solvers can solve/parse them in the future
Brian:   Right so essentially, someone should be able to create multiple json files relating to different problems and run them through the script all at 
				 once correct?
William: yup
				 for the python math problem generator specifically you might want to have a way to generate problems with it, turn those into the proper json 
				 format, and then be able to send those to the api
Brian:   alright sounds good. i'll start with the generator for now

Current workflow URLs to open
https://vcs.he.net/math-solver/math-site-solvers-brian.corbett
https://vcs.he.net/math-solver/math-solver-issues/-/wikis/home
https://vcs.he.net/robert.loth/mathbot_api_test
  (git clone master -> then switch to 'hard-copy' branch)

LINKS:
repo for the math problem generator
	https://github.com/lukew3/mathgenerator
vcs setup utility - by Bob
	https://vcs.he.net/robert.loth/make-setup
API for the math solver (hard-copy branch to avoid access issues)
	https://vcs.he.net/robert.loth/mathbot_api_test
site for the math solver
	https://math.he.net/interact
URL to send data for the API
	https://math.he.net/mathbot

ASKED TO BOB ON 1/2
well looks like I can run it now, i'm getting this error:
		Command '['info-tag/extract-tag.php', '-c', 'he-info']' returned non-zero exit status 127.
Any idea what this means? I've been trying to find out but haven't found anything useful i think
	ANSWER: run this command 'sudo apt install php php-dom php-mbstring' 

-------------------------------------------------
CHATS - From William
Will: https://github.com/lukew3/mathgenerator this is the python library which has a function to generate example problems of a certain type based on a number which indicates the type of problem
Will: https://staging.mathanswered.com/ enter them here not sure if you have an account for the math site yet but you should make one
Will: theres about a 100 unique types or so listed on the github page, generate atleast one problem of each type and make a report of whether our mathbot can solve it or not
12/28, 12:31
Will: cool the report looks good so far
Will: next thing would be to have another script to run each of the test problems with jasons api and put the results in another file
	and then i know bob mightve wrote a tool to make comparing the answers possible
Will: but for now you might want to reformat how you store the questions in solver_capability_tracker.txt so that you can read them again in 
  	python and query the api for each question
Me: gotcha, that's related to a question I had. the solver should be able to parse and solve the problems as they're asked in the python 
  	mathgenerator library right? like in word problem form?
Will: yeah mostly
	for the questions where you had to rewrite the problem a bit to get it to work, you can still list those in the file, and test each of 
 	them against the api
Will: but mostly you should just take the output of the mathgenerator and test that
Will: directly passing to the api
- Links 
https://github.com/lukew3/mathgenerator
https://staging.mathanswered.com/

Setup
- robert.loth
the "Make Setup" script at https://vcs.he.net/robert.loth/make-setup should be much easier to use now, if anyone is still using 
  it. The old version is at tag v0.1 "git checkout v0.1" and the new version is at tag v0.2 "git checkout v0.2". Usage has changed, 
  the first and only argument is the <username> from <username>@he.net 

https://math.he.net/interact
https://math.he.net/mathbot

Math API
12/22, 11:41 (and after 12/22 is when talk of the API begins)
curl -v -F 'q=1+2+3+4'  https://math.he.net/mathbot

12/28, 
https://vcs.he.net/robert.loth/mathbot_api_test
https://docs.python.org/3/library/argparse.html#argparse.ArgumentParser
robert.loth: ok, the api test script has a new url to reflect what it actually does. https://vcs.he.net/robert.loth/mathbot_api_test 
	It should be feature-complete at this point. Only installation step past what is already on the noc computers is "sudo apt install php php-dom php-mbstring"
  to deal with the php subprocess dependencies.


